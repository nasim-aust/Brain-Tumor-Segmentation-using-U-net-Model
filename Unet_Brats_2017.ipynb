{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet Brats 2017.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasim-aust/Brain-Tumor-Segmentation-using-U-net-Model/blob/master/Unet_Brats_2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "LT7vbAkBEKkX",
        "colab_type": "code",
        "outputId": "0e2e98e8-2529-408d-d263-c5d9ae0c0d30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v-E76FBfEL6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lqpqja3bFG5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install -q matplotlib-venn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jpPCH0gb0wiD",
        "colab_type": "code",
        "outputId": "31614a12-9304-4c1a-d421-e0e42a02a84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "#from skimage import io\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import SimpleITK as sitk\n",
        "from keras.utils import np_utils\n",
        "\n",
        "class Pipeline(object):\n",
        "\n",
        "    \n",
        "    def __init__(self, list_train ,Normalize=True):\n",
        "        self.scans_train = list_train\n",
        "        self.train_im=self.read_scans(Normalize)\n",
        "\n",
        "        \n",
        "    def read_scans(self,Normalize):\n",
        "\n",
        "        train_im=[]\n",
        "        for i in range(len( self.scans_train)):\n",
        "            if i%10==0:\n",
        "                print('iteration [{}]'.format(i))\n",
        "\n",
        "            flair = glob( self.scans_train[i] + '/*_flair.nii.gz')\n",
        "            t2 = glob( self.scans_train[i] + '/*_t2.nii.gz')\n",
        "            gt = glob( self.scans_train[i] + '/*_seg.nii.gz')\n",
        "            t1 = glob( self.scans_train[i] + '/*_t1.nii.gz')\n",
        "            t1c = glob( self.scans_train[i] + '/*_t1ce.nii.gz')\n",
        "\n",
        "            t1s=[scan for scan in t1 if scan not in t1c]\n",
        "\n",
        "            if (len(flair)+len(t2)+len(gt)+len(t1s)+len(t1c))<5:\n",
        "                print(\"there is a problem here!!! the problem lies in this patient :\", self.scans_train[i])\n",
        "                continue\n",
        "            scans = [flair[0], t1s[0], t1c[0], t2[0], gt[0]]\n",
        "            \n",
        "            #read a volume composed of 4 modalities\n",
        "            tmp = [sitk.GetArrayFromImage(sitk.ReadImage(scans[k])) for k in range(len(scans))]\n",
        "\n",
        "            #crop each volume to have a size of (146,192,152) to discard some unwanted background and thus save some computational power ;)\n",
        "            z0=1\n",
        "            y0=29\n",
        "            x0=42\n",
        "            z1=147\n",
        "            y1=221  \n",
        "            x1=194  \n",
        "            tmp=np.array(tmp)\n",
        "            tmp=tmp[:,z0:z1,y0:y1,x0:x1]\n",
        "\n",
        "            #normalize each slice\n",
        "            if Normalize==True:\n",
        "                tmp=self.norm_slices(tmp)\n",
        "\n",
        "            train_im.append(tmp)\n",
        "            del tmp    \n",
        "        return  np.array(train_im)\n",
        "    \n",
        "    \n",
        "    def sample_patches_randomly(self, num_patches, d , h , w ):\n",
        "\n",
        "        '''\n",
        "        INPUT:\n",
        "        num_patches : the total number of samled patches\n",
        "        d : this correspnds to the number of channels which is ,in our case, 4 MRI modalities\n",
        "        h : height of the patch\n",
        "        w : width of the patch\n",
        "        OUTPUT:\n",
        "        patches : np array containing the randomly sampled patches\n",
        "        labels : np array containing the corresping target patches\n",
        "        '''\n",
        "        patches, labels = [], []\n",
        "        count = 0\n",
        "\n",
        "        #swap axes to make axis 0 represents the modality and axis 1 represents the slice. take the ground truth\n",
        "        gt_im = np.swapaxes(self.train_im, 0, 1)[4]   \n",
        "\n",
        "        #take flair image as mask\n",
        "        msk = np.swapaxes(self.train_im, 0, 1)[0]\n",
        "        #save the shape of the grounf truth to use it afterwards\n",
        "        tmp_shp = gt_im.shape\n",
        "\n",
        "        #reshape the mask and the ground truth to 1D array\n",
        "        gt_im = gt_im.reshape(-1).astype(np.uint8)\n",
        "        msk = msk.reshape(-1).astype(np.float32)\n",
        "\n",
        "        # maintain list of 1D indices while discarding 0 intensities\n",
        "        indices = np.squeeze(np.argwhere((msk!=-9.0) & (msk!=0.0)))\n",
        "        del msk\n",
        "\n",
        "        # shuffle the list of indices of the class\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        #reshape gt_im\n",
        "        gt_im = gt_im.reshape(tmp_shp)\n",
        "\n",
        "        #a loop to sample the patches from the images\n",
        "        i = 0\n",
        "        pix = len(indices)\n",
        "        while (count<num_patches) and (pix>i):\n",
        "            #randomly choose an index\n",
        "            ind = indices[i]\n",
        "            i+= 1\n",
        "            #reshape ind to 3D index\n",
        "            ind = np.unravel_index(ind, tmp_shp)\n",
        "            # get the patient and the slice id\n",
        "            patient_id = ind[0]\n",
        "            slice_idx=ind[1]\n",
        "            p = ind[2:]\n",
        "            #construct the patch by defining the coordinates\n",
        "            p_y = (p[0] - (h)/2, p[0] + (h)/2)\n",
        "            p_x = (p[1] - (w)/2, p[1] + (w)/2)\n",
        "            p_x=list(map(int,p_x))\n",
        "            p_y=list(map(int,p_y))\n",
        "            \n",
        "            #take patches from all modalities and group them together\n",
        "            tmp = self.train_im[patient_id][0:4, slice_idx,p_y[0]:p_y[1], p_x[0]:p_x[1]]\n",
        "            #take the coresponding label patch\n",
        "            lbl=gt_im[patient_id,slice_idx,p_y[0]:p_y[1], p_x[0]:p_x[1]]\n",
        "\n",
        "            #keep only paches that have the desired size\n",
        "            if tmp.shape != (d, h, w) :\n",
        "                continue\n",
        "            patches.append(tmp)\n",
        "            labels.append(lbl)\n",
        "            count+=1\n",
        "        patches = np.array(patches)\n",
        "        labels=np.array(labels)\n",
        "        return patches, labels\n",
        "        \n",
        "        \n",
        "\n",
        "    def norm_slices(self,slice_not): \n",
        "        '''\n",
        "            normalizes each slice , excluding gt\n",
        "            subtracts mean and div by std dev for each slice\n",
        "            clips top and bottom one percent of pixel intensities\n",
        "        '''\n",
        "        normed_slices = np.zeros(( 5,146, 192, 152)).astype(np.float32)\n",
        "        for slice_ix in range(4):\n",
        "            normed_slices[slice_ix] = slice_not[slice_ix]\n",
        "            for mode_ix in range(146):\n",
        "                normed_slices[slice_ix][mode_ix] = self._normalize(slice_not[slice_ix][mode_ix])\n",
        "        normed_slices[-1]=slice_not[-1]\n",
        "\n",
        "        return normed_slices    \n",
        "   \n",
        "\n",
        "\n",
        "    def _normalize(self,slice):\n",
        "        '''\n",
        "            input: unnormalized slice \n",
        "            OUTPUT: normalized clipped slice\n",
        "        '''\n",
        "        b = np.percentile(slice, 99)\n",
        "        t = np.percentile(slice, 1)\n",
        "        slice = np.clip(slice, t, b)\n",
        "        image_nonzero = slice[np.nonzero(slice)]\n",
        "        if np.std(slice)==0 or np.std(image_nonzero) == 0:\n",
        "            return slice\n",
        "        else:\n",
        "            tmp= (slice - np.mean(image_nonzero)) / np.std(image_nonzero)\n",
        "            #since the range of intensities is between 0 and 5000 ,the min in the normalized slice corresponds to 0 intensity in unnormalized slice\n",
        "            #the min is replaced with -9 just to keep track of 0 intensities so that we can discard those intensities afterwards when sampling random patches\n",
        "            tmp[tmp==tmp.min()]=-9\n",
        "            return tmp\n",
        "\n",
        "\n",
        "'''\n",
        "def save_image_png (img,output_file=\"img.png\"):\n",
        "    \"\"\"\n",
        "    save 2d image to disk in a png format\n",
        "    \"\"\"\n",
        "    img=np.array(img).astype(np.float32)\n",
        "    if np.max(img) != 0:\n",
        "        img /= np.max(img)   # set values < 1                  \n",
        "    if np.min(img) <= -1: # set values > -1\n",
        "        img /= abs(np.min(img))\n",
        "    io.imsave(output_file, img)\n",
        "'''\n",
        "\n",
        "\n",
        "    \n",
        "def concatenate ():\n",
        "\n",
        "    '''\n",
        "    concatenate two parts into one dataset\n",
        "    this can be avoided if there is enough RAM as we can directly from the whole dataset\n",
        "    '''\n",
        "    Y_labels_2=np.load(\"y_dataset_second_part.npy\").astype(np.uint8)\n",
        "    X_patches_2=np.load(\"x_dataset_second_part.npy\").astype(np.float32)\n",
        "    Y_labels_1=np.load(\"y_dataset_first_part.npy\").astype(np.uint8)\n",
        "    X_patches_1=np.load(\"x_dataset_first_part.npy\").astype(np.float32)\n",
        "\n",
        "    #concatenate both parts\n",
        "    X_patches=np.concatenate((X_patches_1, X_patches_2), axis=0)\n",
        "    Y_labels=np.concatenate((Y_labels_1, Y_labels_2), axis=0)\n",
        "    del Y_labels_2,X_patches_2,Y_labels_1,X_patches_1\n",
        "\n",
        "    #shuffle the whole dataset\n",
        "    shuffle = list(zip(X_patches, Y_labels))\n",
        "    np.random.seed(138)\n",
        "    np.random.shuffle(shuffle)\n",
        "    X_patches = np.array([shuffle[i][0] for i in range(len(shuffle))])\n",
        "    Y_labels = np.array([shuffle[i][1] for i in range(len(shuffle))])\n",
        "    del shuffle\n",
        "\n",
        "\n",
        "    np.save( \"x_training\",X_patches.astype(np.float32) )\n",
        "    np.save( \"y_training\",Y_labels.astype(np.uint8))\n",
        "    #np.save( \"x_valid\",X_patches_valid.astype(np.float32) )\n",
        "    #np.save( \"y_valid\",Y_labels_valid.astype(np.uint8))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #Paths for Brats2017 dataset\n",
        "    \n",
        "    path_HGG = glob('drive/Colab Notebooks/BrainTumorSegmentation-Unet/Brats2017/Brats17TrainingData/HGG/**')\n",
        "    path_LGG = glob('drive/Colab Notebooks/BrainTumorSegmentation-Unet/Brats2017/Brats17TrainingData/LGG/**') \n",
        "    #path_HGG = glob('Brats2017/Brats17TrainingData/HGG/**')\n",
        "    #path_LGG = glob('Brats2017/Brats17TrainingData/LGG/**')\n",
        "    path_all=path_HGG+path_LGG\n",
        "\n",
        "    #shuffle the dataset\n",
        "    np.random.seed(2022)\n",
        "    np.random.shuffle(path_all)\n",
        "\n",
        "    np.random.seed(1555)\n",
        "    start=0\n",
        "    end=20\n",
        "    #set the total number of patches\n",
        "    #this formula extracts approximately 3 patches per slice\n",
        "    num_patches=146*(end-start)*3\n",
        "    #define the size of a patch\n",
        "    h=128\n",
        "    w=128 \n",
        "    d=4 \n",
        "\n",
        "    pipe=Pipeline(list_train=path_all[start:end],Normalize=True)\n",
        "    Patches,Y_labels=pipe.sample_patches_randomly(num_patches,d, h, w)\n",
        "\n",
        "    #transform the data to channels_last keras format\n",
        "    Patches=np.transpose(Patches,(0,2,3,1)).astype(np.float32)\n",
        "\n",
        "    # since the brats2017 dataset has only 4 labels,namely 0,1,2 and 4 as opposed to previous datasets \n",
        "    # this transormation is done so that we will have 4 classes when we one-hot encode the targets\n",
        "    Y_labels[Y_labels==4]=3\n",
        "\n",
        "    #transform y to one_hot enconding for keras  \n",
        "    shp=Y_labels.shape[0]\n",
        "    Y_labels=Y_labels.reshape(-1)\n",
        "    Y_labels = np_utils.to_categorical(Y_labels).astype(np.uint8)\n",
        "    Y_labels=Y_labels.reshape(shp,h,w,4)\n",
        "\n",
        "    #shuffle the whole dataset\n",
        "    shuffle = list(zip(Patches, Y_labels))\n",
        "    np.random.seed(180)\n",
        "    np.random.shuffle(shuffle)\n",
        "    Patches = np.array([shuffle[i][0] for i in range(len(shuffle))])\n",
        "    Y_labels = np.array([shuffle[i][1] for i in range(len(shuffle))])\n",
        "    del shuffle\n",
        "    \n",
        "    print(\"Size of the patches : \",Patches.shape)\n",
        "    print(\"Size of their correponding targets : \",Y_labels.shape)\n",
        "\n",
        "    #save to disk as npy files\n",
        "    #np.save( \"x_dataset_first_part\",Patches )\n",
        "    #np.save( \"y_dataset_first_part\",Y_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration [0]\n",
            "iteration [10]\n",
            "Size of the patches :  (8760, 128, 128, 4)\n",
            "Size of their correponding targets :  (8760, 128, 128, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GSAfJKc70wiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#losses.py\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def dice(y_true, y_pred):\n",
        "    #computes the dice score on two tensors\n",
        "\n",
        "    sum_p=K.sum(y_pred,axis=0)\n",
        "    sum_r=K.sum(y_true,axis=0)\n",
        "    sum_pr=K.sum(y_true * y_pred,axis=0)\n",
        "    dice_numerator =2*sum_pr\n",
        "    dice_denominator =sum_r+sum_p\n",
        "    dice_score =(dice_numerator+K.epsilon() )/(dice_denominator+K.epsilon())\n",
        "    return dice_score\n",
        "\n",
        "\n",
        "def dice_whole_metric(y_true, y_pred):\n",
        "    #computes the dice for the whole tumor\n",
        "\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    y_whole=K.sum(y_true_f[:,1:],axis=1)\n",
        "    p_whole=K.sum(y_pred_f[:,1:],axis=1)\n",
        "    dice_whole=dice(y_whole,p_whole)\n",
        "    return dice_whole\n",
        "\n",
        "def dice_en_metric(y_true, y_pred):\n",
        "    #computes the dice for the enhancing region\n",
        "\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    y_enh=y_true_f[:,-1]\n",
        "    p_enh=y_pred_f[:,-1]\n",
        "    dice_en=dice(y_enh,p_enh)\n",
        "    return dice_en\n",
        "\n",
        "def dice_core_metric(y_true, y_pred):\n",
        "    ##computes the dice for the core region\n",
        "\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    \n",
        "    #workaround for tf\n",
        "    #y_core=K.sum(tf.gather(y_true_f, [1,3],axis =1),axis=1)\n",
        "    #p_core=K.sum(tf.gather(y_pred_f, [1,3],axis =1),axis=1)\n",
        "    \n",
        "    y_core=K.sum(y_true_f[:,[1,3]],axis=1)\n",
        "    p_core=K.sum(y_pred_f[:,[1,3]],axis=1)\n",
        "    dice_core=dice(y_core,p_core)\n",
        "    return dice_core\n",
        "\n",
        "\n",
        "\n",
        "def weighted_log_loss(y_true, y_pred):\n",
        "    # scale predictions so that the class probas of each sample sum to 1\n",
        "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "    # clip to prevent NaN's and Inf's\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "    # weights are assigned in this order : normal,necrotic,edema,enhancing \n",
        "    weights=np.array([1,5,2,4])\n",
        "    weights = K.variable(weights)\n",
        "    loss = y_true * K.log(y_pred) * weights\n",
        "    loss = K.mean(-K.sum(loss, -1))\n",
        "    return loss\n",
        "\n",
        "def gen_dice_loss(y_true, y_pred):\n",
        "    '''\n",
        "    computes the sum of two losses : generalised dice loss and weighted cross entropy\n",
        "    '''\n",
        "\n",
        "    #generalised dice score is calculated as in this paper : https://arxiv.org/pdf/1707.03237\n",
        "    y_true_f = K.reshape(y_true,shape=(-1,4))\n",
        "    y_pred_f = K.reshape(y_pred,shape=(-1,4))\n",
        "    sum_p=K.sum(y_pred_f,axis=-2)\n",
        "    sum_r=K.sum(y_true_f,axis=-2)\n",
        "    sum_pr=K.sum(y_true_f * y_pred_f,axis=-2)\n",
        "    weights=K.pow(K.square(sum_r)+K.epsilon(),-1)\n",
        "    generalised_dice_numerator =2*K.sum(weights*sum_pr)\n",
        "    generalised_dice_denominator =K.sum(weights*(sum_r+sum_p))\n",
        "    generalised_dice_score =generalised_dice_numerator /generalised_dice_denominator\n",
        "    GDL=1-generalised_dice_score\n",
        "    del sum_p,sum_r,sum_pr,weights\n",
        "\n",
        "    return GDL+weighted_log_loss(y_true,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7SzLy9W0wiJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model\n",
        "import numpy as np\n",
        "from keras.models import Model,load_model\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dropout,GaussianNoise, Input,Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import  Conv2DTranspose,UpSampling2D,concatenate,add\n",
        "from keras.optimizers import SGD\n",
        "import keras.backend as K\n",
        "#from losses import *\n",
        "\n",
        "K.set_image_data_format(\"channels_last\")\n",
        "\n",
        " #u-net model\n",
        "class Unet_model(object):\n",
        "    \n",
        "    def __init__(self,img_shape,load_model_weights=None):\n",
        "        self.img_shape=img_shape\n",
        "        self.load_model_weights=load_model_weights\n",
        "        self.model =self.compile_unet()\n",
        "        \n",
        "    \n",
        "    def compile_unet(self):\n",
        "        \"\"\"\n",
        "        compile the U-net model\n",
        "        \"\"\"\n",
        "        i = Input(shape=self.img_shape)\n",
        "        #add gaussian noise to the first layer to combat overfitting\n",
        "        i_=GaussianNoise(0.01)(i)\n",
        "\n",
        "        i_ = Conv2D(64, 2, padding='same',data_format = 'channels_last')(i_)\n",
        "        out=self.unet(inputs=i_)\n",
        "        model = Model(input=i, output=out)\n",
        "\n",
        "        sgd = SGD(lr=0.08, momentum=0.9, decay=5e-6, nesterov=False)\n",
        "        model.compile(loss=gen_dice_loss, optimizer=sgd, metrics=[dice_whole_metric,dice_core_metric,dice_en_metric])\n",
        "        #load weights if set for prediction\n",
        "        if self.load_model_weights is not None:\n",
        "            model.load_weights(self.load_model_weights)\n",
        "        return model\n",
        "\n",
        "\n",
        "    def unet(self,inputs, nb_classes=4, start_ch=64, depth=3, inc_rate=2. ,activation='relu', dropout=0.0, batchnorm=True, upconv=True,format_='channels_last'):\n",
        "        \"\"\"\n",
        "        the actual u-net architecture\n",
        "        \"\"\"\n",
        "        o = self.level_block(inputs,start_ch, depth, inc_rate,activation, dropout, batchnorm, upconv,format_)\n",
        "        o = BatchNormalization()(o) \n",
        "        #o =  Activation('relu')(o)\n",
        "        o=PReLU(shared_axes=[1, 2])(o)\n",
        "        o = Conv2D(nb_classes, 1, padding='same',data_format = format_)(o)\n",
        "        o = Activation('softmax')(o)\n",
        "        return o\n",
        "\n",
        "\n",
        "\n",
        "    def level_block(self,m, dim, depth, inc, acti, do, bn, up,format_=\"channels_last\"):\n",
        "        if depth > 0:\n",
        "            n = self.res_block_enc(m,0.0,dim,acti, bn,format_)\n",
        "            #using strided 2D conv for donwsampling\n",
        "            m = Conv2D(int(inc*dim), 2,strides=2, padding='same',data_format = format_)(n)\n",
        "            m = self.level_block(m,int(inc*dim), depth-1, inc, acti, do, bn, up )\n",
        "            if up:\n",
        "                m = UpSampling2D(size=(2, 2),data_format = format_)(m)\n",
        "                m = Conv2D(dim, 2, padding='same',data_format = format_)(m)\n",
        "            else:\n",
        "                m = Conv2DTranspose(dim, 3, strides=2,padding='same',data_format = format_)(m)\n",
        "            n=concatenate([n,m])\n",
        "            #the decoding path\n",
        "            m = self.res_block_dec(n, 0.0,dim, acti, bn, format_)\n",
        "        else:\n",
        "            m = self.res_block_enc(m, 0.0,dim, acti, bn, format_)\n",
        "        return m\n",
        "\n",
        "  \n",
        "   \n",
        "    def res_block_enc(self,m, drpout,dim,acti, bn,format_=\"channels_last\"):\n",
        "        \n",
        "        \"\"\"\n",
        "        the encoding unit which a residual block\n",
        "        \"\"\"\n",
        "        n = BatchNormalization()(m) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format = format_)(n)\n",
        "                \n",
        "        n = BatchNormalization()(n) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format =format_ )(n)\n",
        "\n",
        "        n=add([m,n]) \n",
        "        \n",
        "        return  n \n",
        "\n",
        "\n",
        "\n",
        "    def res_block_dec(self,m, drpout,dim,acti, bn,format_=\"channels_last\"):\n",
        "\n",
        "        \"\"\"\n",
        "        the decoding unit which a residual block\n",
        "        \"\"\"\n",
        "         \n",
        "        n = BatchNormalization()(m) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format = format_)(n)\n",
        "\n",
        "        n = BatchNormalization()(n) if bn else n\n",
        "        #n=  Activation(acti)(n)\n",
        "        n=PReLU(shared_axes=[1, 2])(n)\n",
        "        n = Conv2D(dim, 3, padding='same',data_format =format_ )(n)\n",
        "        \n",
        "        Save = Conv2D(dim, 1, padding='same',data_format = format_,use_bias=False)(m) \n",
        "        n=add([Save,n]) \n",
        "        \n",
        "        return  n   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwLVGIZE0wiK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8y5Y0h3A0wiM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tgkv48T60wiP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evaluation_metrices\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "\n",
        "def binary_dice3d(s,g):\n",
        "    #dice score of two 3D volumes\n",
        "    num=np.sum(np.multiply(s, g))\n",
        "    denom=s.sum() + g.sum() \n",
        "    if denom==0:\n",
        "        return 1\n",
        "    else:\n",
        "        return  2.0*num/denom\n",
        "\n",
        "\n",
        "def sensitivity (seg,ground): \n",
        "    #computs false negative rate\n",
        "    num=np.sum(np.multiply(ground, seg ))\n",
        "    denom=np.sum(ground)\n",
        "    if denom==0:\n",
        "        return 1\n",
        "    else:\n",
        "        return  num/denom\n",
        "\n",
        "def specificity (seg,ground): \n",
        "    #computes false positive rate\n",
        "    num=np.sum(np.multiply(ground==0, seg ==0))\n",
        "    denom=np.sum(ground==0)\n",
        "    if denom==0:\n",
        "        return 1\n",
        "    else:\n",
        "        return  num/denom\n",
        "\n",
        "\n",
        "\n",
        "def border_map(binary_img,neigh):\n",
        "    \"\"\"\n",
        "    Creates the border for a 3D image\n",
        "    \"\"\"\n",
        "    binary_map = np.asarray(binary_img, dtype=np.uint8)\n",
        "    neigh = neigh\n",
        "    west = ndimage.shift(binary_map, [-1, 0,0], order=0)\n",
        "    east = ndimage.shift(binary_map, [1, 0,0], order=0)\n",
        "    north = ndimage.shift(binary_map, [0, 1,0], order=0)\n",
        "    south = ndimage.shift(binary_map, [0, -1,0], order=0)\n",
        "    top = ndimage.shift(binary_map, [0, 0, 1], order=0)\n",
        "    bottom = ndimage.shift(binary_map, [0, 0, -1], order=0)\n",
        "    cumulative = west + east + north + south + top + bottom\n",
        "    border = ((cumulative < 6) * binary_map) == 1\n",
        "    return border\n",
        "\n",
        "\n",
        "def border_distance(ref,seg):\n",
        "    \"\"\"\n",
        "    This functions determines the map of distance from the borders of the\n",
        "    segmentation and the reference and the border maps themselves\n",
        "    \"\"\"\n",
        "    neigh=8\n",
        "    border_ref = border_map(ref,neigh)\n",
        "    border_seg = border_map(seg,neigh)\n",
        "    oppose_ref = 1 - ref\n",
        "    oppose_seg = 1 - seg\n",
        "    # euclidean distance transform\n",
        "    distance_ref = ndimage.distance_transform_edt(oppose_ref)\n",
        "    distance_seg = ndimage.distance_transform_edt(oppose_seg)\n",
        "    distance_border_seg = border_ref * distance_seg\n",
        "    distance_border_ref = border_seg * distance_ref\n",
        "    return distance_border_ref, distance_border_seg#, border_ref, border_seg\n",
        "\n",
        "def Hausdorff_distance(ref,seg):\n",
        "    \"\"\"\n",
        "    This functions calculates the average symmetric distance and the\n",
        "    hausdorff distance between a segmentation and a reference image\n",
        "    :return: hausdorff distance and average symmetric distance\n",
        "    \"\"\"\n",
        "    ref_border_dist, seg_border_dist = border_distance(ref,seg)\n",
        "    hausdorff_distance = np.max(\n",
        "        [np.max(ref_border_dist), np.max(seg_border_dist)])\n",
        "    return hausdorff_distance\n",
        "\n",
        "\n",
        "\n",
        "def DSC_whole(pred, orig_label):\n",
        "    #computes dice for the whole tumor\n",
        "    return binary_dice3d(pred>0,orig_label>0)\n",
        "\n",
        "\n",
        "def DSC_en(pred, orig_label):\n",
        "    #computes dice for enhancing region\n",
        "    return binary_dice3d(pred==4,orig_label==4)\n",
        "\n",
        "\n",
        "def DSC_core(pred, orig_label):\n",
        "    #computes dice for core region\n",
        "    seg_=np.copy(pred)\n",
        "    ground_=np.copy(orig_label)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return binary_dice3d(seg_>0,ground_>0)\n",
        "\n",
        "\n",
        "\n",
        "def sensitivity_whole (seg,ground):\n",
        "    return sensitivity(seg>0,ground>0)\n",
        "\n",
        "def sensitivity_en (seg,ground):\n",
        "    return sensitivity(seg==4,ground==4)\n",
        "\n",
        "def sensitivity_core (seg,ground):\n",
        "    seg_=np.copy(seg)\n",
        "    ground_=np.copy(ground)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return sensitivity(seg_>0,ground_>0)\n",
        "\n",
        "\n",
        "\n",
        "def specificity_whole (seg,ground):\n",
        "    return specificity(seg>0,ground>0)\n",
        "\n",
        "def specificity_en (seg,ground):\n",
        "    return specificity(seg==4,ground==4)\n",
        "\n",
        "def specificity_core (seg,ground):\n",
        "    seg_=np.copy(seg)\n",
        "    ground_=np.copy(ground)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return specificity(seg_>0,ground_>0)\n",
        "    \n",
        "\n",
        "def hausdorff_whole (seg,ground):\n",
        "    return Hausdorff_distance(seg==0,ground==0)\n",
        "\n",
        "def hausdorff_en (seg,ground):\n",
        "    return Hausdorff_distance(seg!=4,ground!=4)\n",
        "\n",
        "def hausdorff_core (seg,ground):\n",
        "    seg_=np.copy(seg)\n",
        "    ground_=np.copy(ground)\n",
        "    seg_[seg_==2]=0\n",
        "    ground_[ground_==2]=0\n",
        "    return Hausdorff_distance(seg_==0,ground_==0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "183XdFmWMXkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Activation, Lambda\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import Concatenate\n",
        "import scipy\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZHz8XW7w0wiQ",
        "colab_type": "code",
        "outputId": "6faab547-3292-4fed-9e07-c89332674282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        }
      },
      "cell_type": "code",
      "source": [
        "#train\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from glob import glob\n",
        "from keras.models import model_from_json,load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import  ModelCheckpoint,Callback,LearningRateScheduler\n",
        "import keras.backend as K\n",
        "#from model import Unet_model\n",
        "#from losses import *\n",
        "#from keras.utils.visualize_util import plot\n",
        "\n",
        "\n",
        "\n",
        "class SGDLearningRateTracker(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        optimizer = self.model.optimizer\n",
        "        lr = K.get_value(optimizer.lr)\n",
        "        decay = K.get_value(optimizer.decay)\n",
        "        lr=lr/10\n",
        "        decay=decay*10\n",
        "        K.set_value(optimizer.lr, lr)\n",
        "        K.set_value(optimizer.decay, decay)\n",
        "        print('LR changed to:',lr)\n",
        "        print('Decay changed to:',decay)\n",
        "\n",
        "\n",
        "\n",
        "class Training(object):\n",
        "    \n",
        "\n",
        "    def __init__(self, batch_size,nb_epoch,load_model_resume_training=None):\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.nb_epoch = nb_epoch\n",
        "\n",
        "        #loading model from path to resume previous training without recompiling the whole model\n",
        "        if load_model_resume_training is not None:\n",
        "            self.model =load_model(load_model_resume_training,custom_objects={'gen_dice_loss': gen_dice_loss,'dice_whole_metric':dice_whole_metric,'dice_core_metric':dice_core_metric,'dice_en_metric':dice_en_metric})\n",
        "            print(\"pre-trained model loaded!\")\n",
        "        else:\n",
        "            unet =Unet_model(img_shape=(128,128,4))\n",
        "            self.model=unet.model\n",
        "            print(\"U-net CNN compiled!\")\n",
        "\n",
        "                    \n",
        "    def fit_unet(self,X33_train,Y_train,X_patches_valid=None,Y_labels_valid=None):\n",
        "\n",
        "        train_generator=self.img_msk_gen(X33_train,Y_train,9999)\n",
        "        checkpointer = ModelCheckpoint(filepath='brain_segmentation/ResUnet.{epoch:02d}_{val_loss:.3f}.hdf5', verbose=1)\n",
        "        self.model.fit_generator(train_generator,steps_per_epoch=len(X33_train)//self.batch_size,epochs=self.nb_epoch, validation_data=(X_patches_valid,Y_labels_valid),verbose=1, callbacks = [checkpointer,SGDLearningRateTracker()])\n",
        "        #self.model.fit(X33_train,Y_train, epochs=self.nb_epoch,batch_size=self.batch_size,validation_data=(X_patches_valid,Y_labels_valid),verbose=1, callbacks = [checkpointer,SGDLearningRateTracker()])\n",
        "\n",
        "    def img_msk_gen(self,X33_train,Y_train,seed):\n",
        "\n",
        "        '''\n",
        "        a custom generator that performs data augmentation on both patches and their corresponding targets (masks)\n",
        "        '''\n",
        "        datagen = ImageDataGenerator(horizontal_flip=True,data_format=\"channels_last\")\n",
        "        datagen_msk = ImageDataGenerator(horizontal_flip=True,data_format=\"channels_last\")\n",
        "        image_generator = datagen.flow(X33_train,batch_size=4,seed=seed)\n",
        "        y_generator = datagen_msk.flow(Y_train,batch_size=4,seed=seed)\n",
        "        while True:\n",
        "            yield(image_generator.next(), y_generator.next())\n",
        "\n",
        "\n",
        "    def save_model(self, model_name):\n",
        "        '''\n",
        "        INPUT string 'model_name': path where to save model and weights, without extension\n",
        "        Saves current model as json and weights as h5df file\n",
        "        '''\n",
        "\n",
        "        model_tosave = '{}.json'.format(model_name)\n",
        "        weights = '{}.hdf5'.format(model_name)\n",
        "        json_string = self.model.to_json()\n",
        "        self.model.save_weights(weights)\n",
        "        with open(model_tosave, 'w') as f:\n",
        "            json.dump(json_string, f)\n",
        "        print ('Model saved.')\n",
        "\n",
        "    def load_model(self, model_name):\n",
        "        '''\n",
        "        Load a model\n",
        "        INPUT  (1) string 'model_name': filepath to model and weights, not including extension\n",
        "        OUTPUT: Model with loaded weights. can fit on model using loaded_model=True in fit_model method\n",
        "        '''\n",
        "        print ('Loading model {}'.format(model_name))\n",
        "        model_toload = '{}.json'.format(model_name)\n",
        "        weights = '{}.hdf5'.format(model_name)\n",
        "        with open(model_toload) as f:\n",
        "            m = next(f)\n",
        "        model_comp = model_from_json(json.loads(m))\n",
        "        model_comp.load_weights(weights)\n",
        "        print ('Model loaded.')\n",
        "        self.model = model_comp\n",
        "        return model_comp\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #set arguments\n",
        "\n",
        "    #reload already trained model to resume training\n",
        "    #model_to_load=\"Models/ResUnet.04_0.646.hdf5\"\n",
        "    model_to_load=\"drive/Colab Notebooks/BrainTumorSegmentation-Unet/Models/ResUnet.04_0.646.hdf5\"\n",
        "    \n",
        "    #save=None\n",
        "\n",
        "    #compile the model\n",
        "    brain_seg = Training(batch_size=4,nb_epoch=3,load_model_resume_training=model_to_load)\n",
        "\n",
        "    print(\"number of trainabale parameters:\",brain_seg.model.count_params())\n",
        "    #  print(brain_seg.model.summary())\n",
        "    #plot(brain_seg.model, to_file='model_architecture.png', show_shapes=True)\n",
        "\n",
        "    #load data from disk\n",
        "    Y_labels=np.load(\"y_training.npy\").astype(np.uint8)\n",
        "    X_patches=np.load(\"x_training.npy\").astype(np.float32)\n",
        "    Y_labels_valid=np.load(\"y_valid.npy\").astype(np.uint8)\n",
        "    X_patches_valid=np.load(\"x_valid.npy\").astype(np.float32)\n",
        "    print(\"loading patches done\\n\")\n",
        "\n",
        "    # fit model\n",
        "    brain_seg.fit_unet(X_patches,Y_labels,X_patches_valid,Y_labels_valid)#*\n",
        "\n",
        "    #if save is not None:\n",
        "     #   brain_seg.save_model('models/' + save)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-4b8ca074f3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m#compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mbrain_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_model_resume_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_to_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number of trainabale parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrain_seg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-4b8ca074f3e0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, nb_epoch, load_model_resume_training)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#loading model from path to resume previous training without recompiling the whole model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_model_resume_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model_resume_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'gen_dice_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen_dice_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dice_whole_metric'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdice_whole_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dice_core_metric'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdice_core_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dice_en_metric'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdice_en_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre-trained model loaded!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot create group in read only mode.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot create group in read only mode."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Fi5GxMzp0wiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from glob import glob\n",
        "import os\n",
        "import SimpleITK as sitk\n",
        "from evaluation_metrics import *\n",
        "from model import Unet_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Prediction(object):\n",
        "\n",
        "    def __init__(self, batch_size_test,load_model_path):\n",
        "\n",
        "        self.batch_size_test=batch_size_test\n",
        "        unet=Unet_model(img_shape=(240,240,4),load_model_weights=load_model_path)\n",
        "        self.model=unet.model\n",
        "        print ('U-net CNN compiled!\\n')\n",
        "\n",
        "\n",
        "    def predict_volume(self, filepath_image,show):\n",
        "\n",
        "        '''\n",
        "        segment the input volume\n",
        "        INPUT   (1) str 'filepath_image': filepath of the volume to predict \n",
        "                (2) bool 'show': True to ,\n",
        "        OUTPUt  (1) np array of the predicted volume\n",
        "                (2) np array of the corresping ground truth\n",
        "        '''\n",
        "\n",
        "        #read the volume\n",
        "        flair = glob( filepath_image + '/*_flair.nii.gz')\n",
        "        t2 = glob( filepath_image + '/*_t2.nii.gz')\n",
        "        gt = glob( filepath_image + '/*_seg.nii.gz')\n",
        "        t1s = glob( filepath_image + '/*_t1.nii.gz')\n",
        "        t1c = glob( filepath_image + '/*_t1ce.nii.gz')\n",
        "        t1=[scan for scan in t1s if scan not in t1c]\n",
        "        if (len(flair)+len(t2)+len(gt)+len(t1)+len(t1c))<5:\n",
        "            print(\"there is a problem here!!! the problem lies in this patient :\")\n",
        "        scans_test = [flair[0], t1[0], t1c[0], t2[0], gt[0]]\n",
        "        test_im = [sitk.GetArrayFromImage(sitk.ReadImage(scans_test[i])) for i in range(len(scans_test))]\n",
        "\n",
        "\n",
        "        test_im=np.array(test_im).astype(np.float32)\n",
        "        test_image = test_im[0:4]\n",
        "        gt=test_im[-1]\n",
        "        gt[gt==4]=3\n",
        "\n",
        "        #normalize each slice following the same scheme used for training\n",
        "        test_image=self.norm_slices(test_image)\n",
        "        \n",
        "        #transform teh data to channels_last keras format\n",
        "        test_image = test_image.swapaxes(0,1)\n",
        "        test_image=np.transpose(test_image,(0,2,3,1))\n",
        "\n",
        "        if show:\n",
        "            verbose=1\n",
        "        else:\n",
        "            verbose=0\n",
        "        # predict classes of each pixel based on the model\n",
        "        prediction = self.model.predict(test_image,batch_size=self.batch_size_test,verbose=verbose)   \n",
        "        prediction = np.argmax(prediction, axis=-1)\n",
        "        prediction=prediction.astype(np.uint8)\n",
        "        #reconstruct the initial target values .i.e. 0,1,2,4 for prediction and ground truth\n",
        "        prediction[prediction==3]=4\n",
        "        gt[gt==3]=4\n",
        "        \n",
        "        return np.array(prediction),np.array(gt)\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_segmented_volume(self, filepath_image,save,show,save_path):\n",
        "        '''\n",
        "        computes the evaluation metrics on the segmented volume\n",
        "        INPUT   (1) str 'filepath_image': filepath to test image for segmentation, including file extension\n",
        "                (2) bool 'save': whether to save to disk or not\n",
        "                (3) bool 'show': If true, prints the evaluation metrics\n",
        "        OUTPUT np array of all evaluation metrics\n",
        "        '''\n",
        "        \n",
        "        predicted_images,gt= self.predict_volume(filepath_image,show)\n",
        "\n",
        "        if save:\n",
        "            tmp=sitk.GetImageFromArray(predicted_images)\n",
        "            sitk.WriteImage ( tmp,'predictions/{}.nii.gz'.format(save_path) )\n",
        "\n",
        "        #compute the evaluation metrics \n",
        "        Dice_complete=DSC_whole(predicted_images,gt)\n",
        "        Dice_enhancing=DSC_en(predicted_images,gt)\n",
        "        Dice_core=DSC_core(predicted_images,gt)\n",
        "\n",
        "        Sensitivity_whole=sensitivity_whole(predicted_images,gt)\n",
        "        Sensitivity_en=sensitivity_en(predicted_images,gt)\n",
        "        Sensitivity_core=sensitivity_core(predicted_images,gt)\n",
        "        \n",
        "\n",
        "        Specificity_whole=specificity_whole(predicted_images,gt)\n",
        "        Specificity_en=specificity_en(predicted_images,gt)\n",
        "        Specificity_core=specificity_core(predicted_images,gt)\n",
        "\n",
        "\n",
        "        Hausdorff_whole=hausdorff_whole(predicted_images,gt)\n",
        "        Hausdorff_en=hausdorff_en(predicted_images,gt)\n",
        "        Hausdorff_core=hausdorff_core(predicted_images,gt)\n",
        "\n",
        "        if show:\n",
        "            print(\"************************************************************\")\n",
        "            print(\"Dice complete tumor score : {:0.4f}\".format(Dice_complete))\n",
        "            print(\"Dice core tumor score (tt sauf vert): {:0.4f}\".format(Dice_core))\n",
        "            print(\"Dice enhancing tumor score (jaune):{:0.4f} \".format(Dice_enhancing))\n",
        "            print(\"**********************************************\")\n",
        "            print(\"Sensitivity complete tumor score : {:0.4f}\".format(Sensitivity_whole))\n",
        "            print(\"Sensitivity core tumor score (tt sauf vert): {:0.4f}\".format(Sensitivity_core))\n",
        "            print(\"Sensitivity enhancing tumor score (jaune):{:0.4f} \".format(Sensitivity_en))\n",
        "            print(\"***********************************************\")\n",
        "            print(\"Specificity complete tumor score : {:0.4f}\".format(Specificity_whole))\n",
        "            print(\"Specificity core tumor score (tt sauf vert): {:0.4f}\".format(Specificity_core))\n",
        "            print(\"Specificity enhancing tumor score (jaune):{:0.4f} \".format(Specificity_en))\n",
        "            print(\"***********************************************\")\n",
        "            print(\"Hausdorff complete tumor score : {:0.4f}\".format(Hausdorff_whole))\n",
        "            print(\"Hausdorff core tumor score (tt sauf vert): {:0.4f}\".format(Hausdorff_core))\n",
        "            print(\"Hausdorff enhancing tumor score (jaune):{:0.4f} \".format(Hausdorff_en))\n",
        "            print(\"***************************************************************\\n\\n\")\n",
        "\n",
        "        return np.array((Dice_complete,Dice_core,Dice_enhancing,Sensitivity_whole,Sensitivity_core,Sensitivity_en,Specificity_whole,Specificity_core,Specificity_en,Hausdorff_whole,Hausdorff_core,Hausdorff_en))#))\n",
        "    \n",
        "\n",
        "    def predict_multiple_volumes (self, filepath_volumes,save,show):\n",
        "\n",
        "        results,Ids=[],[]\n",
        "        for patient in filepath_volumes:\n",
        "            tmp1=patient.split('/')\n",
        "            print(\"Volume ID: \" ,tmp1[-2]+'/'+tmp1[-1])\n",
        "            tmp=self.evaluate_segmented_volume(patient,save=save,show=show,save_path=os.path.basename(patient))\n",
        "            #save the results of each volume\n",
        "            results.append(tmp)\n",
        "            #save each ID for later use\n",
        "            Ids.append(str(tmp1[-2]+'/'+tmp1[-1]))\n",
        "\n",
        "        res=np.array(results)     \n",
        "        print(\"mean : \",np.mean(res,axis=0))\n",
        "        print(\"std : \",np.std(res,axis=0))\n",
        "        print(\"median : \",np.median(res,axis=0))\n",
        "        print(\"25 quantile : \",np.percentile(res,25,axis=0))\n",
        "        print(\"75 quantile : \",np.percentile(res,75,axis=0))\n",
        "        print(\"max : \",np.max(res,axis=0))\n",
        "        print(\"min : \",np.min(res,axis=0))\n",
        "\n",
        "        np.savetxt('Results.out', res)\n",
        "        np.savetxt('Volumes_ID.out', Ids,fmt='%s')\n",
        "\n",
        "\n",
        "    def norm_slices(self,slice_not):\n",
        "        '''\n",
        "            normalizes each slice, excluding gt\n",
        "            subtracts mean and div by std dev for each slice\n",
        "            clips top and bottom one percent of pixel intensities\n",
        "        '''\n",
        "        normed_slices = np.zeros(( 4,155, 240, 240))\n",
        "        for slice_ix in range(4):\n",
        "            normed_slices[slice_ix] = slice_not[slice_ix]\n",
        "            for mode_ix in range(155):\n",
        "                normed_slices[slice_ix][mode_ix] = self._normalize(slice_not[slice_ix][mode_ix])\n",
        "\n",
        "        return normed_slices    \n",
        "\n",
        "\n",
        "    def _normalize(self,slice):\n",
        "\n",
        "        b = np.percentile(slice, 99)\n",
        "        t = np.percentile(slice, 1)\n",
        "        slice = np.clip(slice, t, b)\n",
        "        image_nonzero = slice[np.nonzero(slice)]\n",
        "        \n",
        "        if np.std(slice)==0 or np.std(image_nonzero) == 0:\n",
        "            return slice\n",
        "        else:\n",
        "            tmp= (slice - np.mean(image_nonzero)) / np.std(image_nonzero)\n",
        "            tmp[tmp==tmp.min()]=-9\n",
        "            return tmp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #set arguments\n",
        "    model_to_load=\"models_saved/ResUnet.04_0.646.hdf5\" \n",
        "    #paths for the testing data\n",
        "    path_HGG = glob('Brats2017/Brats17TrainingData/HGG/**')\n",
        "    path_LGG = glob('Brats2017/Brats17TrainingData/LGG/**')\n",
        "\n",
        "    test_path=path_HGG+path_LGG\n",
        "    np.random.seed(2022)\n",
        "    np.random.shuffle(test_path)\n",
        "\n",
        "    #compile the model\n",
        "    brain_seg_pred = Prediction(batch_size_test=2 ,load_model_path=model_to_load)\n",
        "\n",
        "    #predicts each volume and save the results in np array\n",
        "    brain_seg_pred.predict_multiple_volumes(test_path[200:290],save=False,show=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}